{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dNNtDUVQau5"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
        "import clip\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import time\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "cS5ouHI3XMIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models\n",
        "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "owlvit_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "owlvit_model.to(device)\n",
        "\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device)\n"
      ],
      "metadata": {
        "id": "_90qRp0dQjSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5RzHv2XORdwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_video_with_owlvit(video_path, text_labels):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps_list = []\n",
        "\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    height, width = frame.shape[:2]\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter('output_with_owlvit.avi', fourcc, 2.0, (width, height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        start = time.time()\n",
        "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Pass the frame and text labels into OwlViT model\n",
        "        inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        outputs = owlvit_model(**inputs)\n",
        "\n",
        "        # Post-processing step\n",
        "        target_sizes = torch.tensor([(image.height, image.width)]).to(device)\n",
        "        results = processor.post_process_grounded_object_detection(\n",
        "            outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n",
        "        )[0]\n",
        "\n",
        "        boxes, scores, proc_text_labels = results[\"boxes\"], results[\"scores\"], results[\"text_labels\"]\n",
        "\n",
        "        # Annotate the frame with bounding boxes and labels using OpenCV (for video output)\n",
        "        for box, score, label in zip(boxes, scores, proc_text_labels):\n",
        "            xmin, ymin, xmax, ymax = map(int, box.tolist())\n",
        "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
        "            cv2.putText(frame, f\"{label}: {score:.2f}\", (xmin, ymin - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "        fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "        ax.imshow(frame)\n",
        "        ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        plt.close(fig)\n",
        "\n",
        "        fps_list.append(1 / (time.time() - start))\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Video saved as 'output_with_owlvit.avi'\")\n",
        "    print(f\"Average FPS: {sum(fps_list)/len(fps_list):.2f}\")"
      ],
      "metadata": {
        "id": "WCBCkdbsQmJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_with_clip(video_path, text_labels):\n",
        "    # Open the video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps_list = []\n",
        "\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    height, width = frame.shape[:2]\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset to first frame\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter('output_with_clip.avi', fourcc, 2.0, (width, height))\n",
        "\n",
        "    # Preprocess text labels\n",
        "    text_inputs = torch.cat([clip.tokenize([txt]) for txt in text_labels]).to(device)\n",
        "\n",
        "    # Process the video frame by frame\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Preprocess image for CLIP\n",
        "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "        # Get image and text features\n",
        "        with torch.no_grad():\n",
        "            image_features = clip_model.encode_image(image_input)\n",
        "            text_features = clip_model.encode_text(text_inputs)\n",
        "\n",
        "        # Normalize features and calculate cosine similarity\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        similarity = (image_features @ text_features.T).squeeze(0)  # Cosine similarity\n",
        "\n",
        "        # Get the top matching text label\n",
        "        top_match_idx = similarity.argmax().item()\n",
        "        top_match_score = similarity[top_match_idx].item()\n",
        "\n",
        "        # Annotate the frame with the top matching label and score\n",
        "        cv2.putText(frame, f\"Top match: {text_labels[top_match_idx]} ({top_match_score:.2f})\",\n",
        "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "        # Display the frame in the notebook (using matplotlib)\n",
        "        fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "        ax.imshow(frame)  # Show the frame with text annotations\n",
        "        ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()  # Display the plot inline\n",
        "\n",
        "        # Write the frame (with annotation) to the video output using OpenCV\n",
        "        out.write(frame)  # Write the frame directly to the output video\n",
        "\n",
        "        plt.close(fig)  # Close the plot to free memory\n",
        "\n",
        "        end = time.time()\n",
        "        fps_list.append(1 / (end - start))\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Video saved as 'output_with_clip.avi'\")\n",
        "    print(f\"Average FPS: {sum(fps_list)/len(fps_list):.2f}\")"
      ],
      "metadata": {
        "id": "SG__zQ_wQnwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/input_vid.avi'\n",
        "text_labels = [[\"matchbox and matchsticks\", \"pc monitor\", \"lion\", \"drone\", \"light bulb\"]]\n",
        "process_video_with_owlvit(video_path, text_labels)\n"
      ],
      "metadata": {
        "id": "9o-fDWWtS9Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_labels = [\"matchbox and matchsticks\", \"pc monitor\", \"lion\", \"drone\", \"light bulb\"]\n",
        "\n",
        "process_video_with_clip(video_path, text_labels)"
      ],
      "metadata": {
        "id": "4DSrTPRLVu4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2QwfIzWmWJbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}